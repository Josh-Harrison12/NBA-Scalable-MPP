{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a8535-2c5f-4ad0-8a02-70bfc0055765",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"git+https://github.com/aresPanos/Interpretable-Point-Processes.git\"\n",
    "!pip install torch\n",
    "!pip install --ignore-installed llvmlite\n",
    "!pip install pomegranate\n",
    "!pip install scipy\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from vi_dpp import VI_model\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from pomegranate import *\n",
    "from scipy.stats import lognorm\n",
    "import torch\n",
    "import time\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.special import logsumexp\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dab4fb-79e4-43d1-8ba2-119880fd7ba2",
   "metadata": {},
   "source": [
    "The Section Below imports the necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cee509-24c1-4f64-9482-957001000858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2022 Data\n",
    "with open(\"timestamps_2022_test.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "times_all = [np.array(value) for value in my_list.values()]\n",
    "type(times_all)\n",
    "print('Event Times:')\n",
    "#print(times_all[:5])\n",
    "\n",
    "\n",
    "with open(\"marklist_2022_test.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "marks_all = [np.array(value) for value in my_list.values()]\n",
    "type(marks_all)\n",
    "print('Mark list:')\n",
    "#print(marks_all[:5])\n",
    "\n",
    "with open(\"teamlist_2022_test.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "teams_all = [np.array(value) for value in my_list.values()]\n",
    "type(teams_all)\n",
    "print('Teams list:')\n",
    "#print(teams_all[:5])\n",
    "\n",
    "with open(\"zonelist_2022_test.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "zones_all = [np.array(value) for value in my_list.values()]\n",
    "type(zones_all)\n",
    "print('Zones list:')\n",
    "#print(zones_all[:5])\n",
    "\n",
    "with open(\"timestamps_2023_final.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "times_all_2023 = [np.array(value) for value in my_list.values()]\n",
    "type(times_all)\n",
    "print('Event Times:')\n",
    "#print(times_all[:5])\n",
    "\n",
    "#2023 Data\n",
    "with open(\"marklist_2023_final.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "marks_all_2023 = [np.array(value) for value in my_list.values()]\n",
    "type(marks_all)\n",
    "print('Mark list:')\n",
    "#print(marks_all[:5])\n",
    "\n",
    "with open(\"teamlist_2023_final.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "teams_all_2023 = [np.array(value) for value in my_list.values()]\n",
    "type(teams_all)\n",
    "print('Teams list:')\n",
    "#print(teams_all[:5])\n",
    "\n",
    "with open(\"zonelist_2023_final.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "zones_all_2023 = [np.array(value) for value in my_list.values()]\n",
    "type(zones_all)\n",
    "print('Zones list:')\n",
    "#print(zones_all[:5])\n",
    "\n",
    "#2022 Data Rebounds Removed\n",
    "with open(\"noreb_timestamps_2022_final.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "noreb_times_all_2022 = [np.array(value) for value in my_list.values()]\n",
    "type(times_all)\n",
    "print('Event Times:')\n",
    "#print(times_all[:5])\n",
    "\n",
    "\n",
    "with open(\"noreb_marklist_2022_final.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "noreb_marks_all_2022 = [np.array(value) for value in my_list.values()]\n",
    "type(marks_all)\n",
    "print('Mark list:')\n",
    "#print(marks_all[:5])\n",
    "\n",
    "with open(\"noreb_teamlist_2022_final.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "noreb_teams_all_2022 = [np.array(value) for value in my_list.values()]\n",
    "type(teams_all)\n",
    "print('Teams list:')\n",
    "#print(teams_all[:5])\n",
    "\n",
    "with open(\"noreb_zonelist_2022_final.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "noreb_zones_all_2022 = [np.array(value) for value in my_list.values()]\n",
    "type(zones_all)\n",
    "print('Zones list:')\n",
    "#print(zones_all[:5])\n",
    "\n",
    "#2023 Data Rebounds Removed\n",
    "with open(\"noreb_timestamps_2023_final.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "noreb_times_all_2023 = [np.array(value) for value in my_list.values()]\n",
    "type(times_all)\n",
    "print('Event Times:')\n",
    "#print(times_all[:5])\n",
    "\n",
    "\n",
    "with open(\"noreb_marklist_2023_final.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "noreb_marks_all_2023 = [np.array(value) for value in my_list.values()]\n",
    "type(marks_all)\n",
    "print('Mark list:')\n",
    "#print(marks_all[:5])\n",
    "\n",
    "with open(\"noreb_teamlist_2023_final.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "noreb_teams_all_2023 = [np.array(value) for value in my_list.values()]\n",
    "type(teams_all)\n",
    "print('Teams list:')\n",
    "#print(teams_all[:5])\n",
    "\n",
    "with open(\"noreb_zonelist_2023_final.json\", \"r\") as file:\n",
    "    my_list = json.load(file)\n",
    "type(my_list)\n",
    "noreb_zones_all_2023 = [np.array(value) for value in my_list.values()]\n",
    "type(zones_all)\n",
    "print('Zones list:')\n",
    "#print(zones_all[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600de89-723e-4825-86bd-169d58ce85ff",
   "metadata": {},
   "source": [
    "The following defines a function for the Training, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "509e7564-0b47-498a-b746-4a93ee16967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training, Valditation and Testing data:\n",
    "seed=2105098\n",
    "def test_train(times_all,marks_all,teams_all,zones_all):\n",
    "    if len(marks_all)==(1230*4):\n",
    "        train_size=80/82\n",
    "        val_size=2/82\n",
    "    else:\n",
    "        train_size=38/40\n",
    "        val_size=2/40\n",
    "    \n",
    "    N = len(times_all)\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    all_idx = np.arange(N)\n",
    "    \n",
    "    train_end = int(train_size * N)\n",
    "    val_end = int((train_size + val_size) * N)\n",
    "    \n",
    "    train_idx = all_idx[:train_end]\n",
    "    val_idx = all_idx[train_end:]\n",
    "    #val_idx = all_idx[train_end:val_end]\n",
    "    #test_idx = all_idx[val_end:]\n",
    "    \n",
    "    times_list_train, marks_list_train,teams_list_train,zones_list_train = [], [],[],[]\n",
    "    times_list_val, marks_list_val,teams_list_val,zones_list_val = [], [],[],[]\n",
    "    #times_list_test, marks_list_test,teams_list_test,zones_list_test = [], [],[],[]\n",
    "    \n",
    "    for idx in train_idx:\n",
    "        times_list_train.append(times_all[idx])\n",
    "        marks_list_train.append(marks_all[idx])\n",
    "        teams_list_train.append(teams_all[idx])\n",
    "        zones_list_train.append(zones_all[idx])\n",
    "    \n",
    "    for idx in val_idx:\n",
    "        times_list_val.append(times_all[idx])\n",
    "        marks_list_val.append(marks_all[idx])\n",
    "        teams_list_val.append(teams_all[idx])\n",
    "        zones_list_val.append(zones_all[idx])\n",
    "    \n",
    "    #for idx in test_idx:\n",
    "       # times_list_test.append(times_all[idx])\n",
    "       # marks_list_test.append(marks_all[idx])\n",
    "       # teams_list_test.append(teams_all[idx])\n",
    "       # zones_list_test.append(zones_all[idx])\n",
    "    \n",
    "    U_dim = max([mark.max() for mark in marks_all]) + 1\n",
    "    num_teams = max([id_vec.max() for id_vec in teams_list_train]) +1\n",
    "    \n",
    "    #print(len(times_list_train),len(times_list_val),len(times_list_test))\n",
    "    #print(len(marks_list_train),len(marks_list_val),len(marks_list_test))\n",
    "    #print(len(teams_list_train),len(teams_list_val),len(teams_list_test))\n",
    "    #print(len(zones_list_train),len(zones_list_val),len(zones_list_test))\n",
    "    print('Length Of Training Data:',len(times_list_train))\n",
    "    print('    Length Of Test Data:',len(times_list_val), '\\n')\n",
    "\n",
    "\n",
    "    return(times_list_train,marks_list_train,zones_list_train,teams_list_train,times_list_val,marks_list_val,zones_list_val,teams_list_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7840ad8-d9e2-4e38-adcb-431196249d1d",
   "metadata": {},
   "source": [
    "Here we have defined the VI Framework and functions for generating times, this is an adapted form of Panos 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbf37c87-6a36-4c49-a019-908268a7ee03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def clamp_preserve_gradients(x: torch.Tensor, min: float, max: float) -> torch.Tensor:\n",
    "    \"\"\"Clamp the tensor while preserving gradients in the clamped region.\"\"\"\n",
    "    return x + (x.clamp(min, max) - x).detach()\n",
    "\n",
    "def tick2myformat(timestamps):\n",
    "    list_times = []\n",
    "    list_marks = []\n",
    "\n",
    "    N = len(timestamps)\n",
    "    U = len(timestamps[0])\n",
    "\n",
    "    for i in range(N):\n",
    "        seq = timestamps[i]\n",
    "\n",
    "        all_times_i = np.concatenate(seq)\n",
    "        all_marks_i = []\n",
    "        for u in range(U):\n",
    "            all_marks_i = np.concatenate((all_marks_i, u*np.ones_like(seq[u], dtype=np.int) ))\n",
    "\n",
    "        arg_ind = np.argsort(all_times_i)\n",
    "\n",
    "        list_times.append(all_times_i[arg_ind])\n",
    "        list_marks.append(all_marks_i[arg_ind].astype(np.int))\n",
    "\n",
    "    return list_times, list_marks\n",
    "\n",
    "\n",
    "class VI_Framework(object):\n",
    "    def __init__(self,\n",
    "                 data_times: list,\n",
    "                 data_marks: list,\n",
    "                 data_zones: list,\n",
    "                 data_teamsID: list,\n",
    "                 data_times_val: list,\n",
    "                 data_marks_val: list,\n",
    "                 data_zones_val: list,\n",
    "                 data_teamsID_val: list,\n",
    "                 cut_off: int,\n",
    "                 num_samples: int=1,\n",
    "                 verbose: bool=False,\n",
    "                 tol=1e-5,\n",
    "                 weight_temp=1.,\n",
    "                 lr_gamma=0.9999,\n",
    "                 max_iter: int=10000,\n",
    "                 interval: int=100,\n",
    "                 print_every: int=100,\n",
    "                 offset: int=0,\n",
    "                 momentum=0.5,\n",
    "                lr_init = 0.03):\n",
    "\n",
    "        if not (isinstance(data_times[0], np.ndarray) and isinstance(data_times_val[0], np.ndarray)):\n",
    "            raise TypeError(\"Invalid 'data_times' or ''data_times_val' provided\")\n",
    "\n",
    "        if not (isinstance(data_marks[0], np.ndarray) and isinstance(data_marks_val[0], np.ndarray)):\n",
    "            raise TypeError(\"Invalid 'data_marks' or 'data_marks_val' provided\")\n",
    "\n",
    "        if not (isinstance(data_zones[0], np.ndarray) and isinstance(data_zones_val[0], np.ndarray)):\n",
    "            raise TypeError(\"Invalid 'data_zones' or'data_zones_val' provided\")\n",
    "\n",
    "        if not (isinstance(data_teamsID[0], np.ndarray) and isinstance(data_teamsID_val[0], np.ndarray)):\n",
    "            raise TypeError(\"Invalid 'data_teamsID' or 'data_teamsID_val' provided\")       \n",
    "\n",
    "        self.tol = tol\n",
    "        self.cut_off = cut_off\n",
    "        self.lr_gamma = lr_gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.num_samples = num_samples\n",
    "        self.interval = interval\n",
    "        self.print_every = print_every\n",
    "        self.offset = offset\n",
    "        self.momentum = momentum\n",
    "        self.weight_temp = weight_temp\n",
    "        self.verbose = verbose\n",
    "        self.lr_init = lr_init\n",
    "\n",
    "        self.U_dim = int(max([mark_vec.max() for mark_vec in data_marks]) + 1)\n",
    "        self.HA_dim = int(self.U_dim/2)\n",
    "        self.Z_dim = int(max([zone_vec.max() for zone_vec in data_zones]) + 1)\n",
    "        self.num_teams = int(max([id_vec.max() for id_vec in data_teamsID]) + 1)\n",
    "        self.size_alpha_betas = int(1 + self.Z_dim * self.U_dim**2)\n",
    "        self.size_phis = int(self.Z_dim * self.U_dim * (self.U_dim - 1))\n",
    "        self.size_deltas = int(self.Z_dim * self.U_dim)\n",
    "        self.data_marks = data_marks\n",
    "        self.data_zones = data_zones\n",
    "        self.data_teamsID = data_teamsID\n",
    "        self.data_times_val = data_times_val\n",
    "        self.data_marks_val = data_marks_val\n",
    "        self.data_zones_val = data_zones_val\n",
    "        self.data_teamsID_val = data_teamsID_val\n",
    "        self.num_seqs = len(data_marks)\n",
    "        self.num_seqs_val = len(data_marks_val)\n",
    "        self.num_jumps = [mark_vec.size for mark_vec in data_marks]\n",
    "        self.num_jumps_val = [mark_vec.size for mark_vec in data_marks_val]\n",
    "        self.elbo_history = np.zeros(self.max_iter)\n",
    "        self.log_sqrt2pi = 0.5 * np.log(2 * np.pi)\n",
    "        self.train_time = 0.\n",
    "        self.u_b = 1 - 1e-6\n",
    "        self.l_b = -1 + 1e-6\n",
    "        self.num_gw = int(self.num_seqs/(self.num_teams*2))\n",
    "\n",
    "        self.m_betas_ind = int(self.size_alpha_betas)\n",
    "        self.sigma_betas_ind = int(2*self.size_alpha_betas)\n",
    "        self.m_phis_ind = int(self.sigma_betas_ind + self.size_phis)\n",
    "        self.sigma_phis_ind = int(self.m_phis_ind + self.size_phis)\n",
    "        self.m_omega_all_ind = int(self.sigma_phis_ind + (self.U_dim-1) * self.num_teams)\n",
    "        self.sigma_omega_all_ind = int(self.m_omega_all_ind + (self.HA_dim) * self.num_teams)\n",
    "        self.rho_ind = int(self.sigma_omega_all_ind + (self.HA_dim) * self.num_teams)\n",
    "        self.omegas_all_ind = int(self.rho_ind + (self.U_dim-1) * self.num_teams * self.num_gw)\n",
    "\n",
    "        self.n_var_params = int(2 * (self.size_alpha_betas + self.size_phis) + self.size_deltas + self.num_teams * ((self.U_dim-1) + (self.HA_dim) + (self.HA_dim) + (self.U_dim-1) * self.num_gw))\n",
    "\n",
    "        self.cache_data(data_times)\n",
    "\n",
    "        self.positive_constraint = torch.nn.Softplus()\n",
    "        self.minus_one_one_constraint = torch.nn.Sigmoid()\n",
    "        self.positive_constraint_np = lambda x: np.log(1. + np.exp(x))\n",
    "        self.coeffs = None\n",
    "        self.coeffs_old = None\n",
    "        self.prior_sigma_betas_alpha = torch.tensor(0.1 * np.ones(self.size_alpha_betas))\n",
    "\n",
    "\n",
    "    def cache_data(self, data_times):\n",
    "        self.tau_ij_minus_list = []\n",
    "        for i in range(self.num_seqs):\n",
    "            vec_times = data_times[i]\n",
    "            N = vec_times.size\n",
    "            minus_tau_ij = np.zeros((self.cut_off, N - 1))\n",
    "            for q in range(self.cut_off):\n",
    "                minus_tau_ij[q, q:N-1] = vec_times[:N-q-1] - vec_times[q+1:N] # change sign - used in likelihood computation\n",
    "\n",
    "            self.tau_ij_minus_list.append(torch.tensor(minus_tau_ij))\n",
    "\n",
    "\n",
    "    def check_convergence(self):\n",
    "        self.max_norm = (self.coeffs - self.coeffs_old).abs().max()\n",
    "        if self.max_norm < self.tol:\n",
    "            return True\n",
    "        self.coeffs_old = self.coeffs.detach().clone()\n",
    "        return False\n",
    "\n",
    "\n",
    "    def callback(self, it, acc, b_acc):\n",
    "        if (it+1) % self.print_every == 0 or it==0:\n",
    "            print('\\riter: {:4d}/{:d} | ELBO: {:.4f} | Test acc: {:.2f}  | Best Test acc: {:.2f} | dx: {:.4f} | time: {:.2f} s' .format(it+1, self.max_iter, self.elbo_history[it], acc, b_acc, self.max_norm, self.train_time))\n",
    "\n",
    "\n",
    "    def pos_constraint(self, input):\n",
    "        res = np.zeros_like(input)\n",
    "        mask = input > 5\n",
    "        res[mask] = input[mask]\n",
    "        res[~mask] = np.log(1. + np.exp(input[~mask]))\n",
    "        return res\n",
    "\n",
    "\n",
    "    def minus_one_one_constraint_np(self, input):\n",
    "        return (2. / (1. + np.exp(-input)) - 1.).clip(min=-1+1e-6, max=1-1e-6)\n",
    "\n",
    "\n",
    "    def compute_acc(self, Q: int=5):\n",
    "        x_opt = self.coeffs.detach().numpy()\n",
    "\n",
    "        size_alpha_betas = 1 + self.Z_dim * self.U_dim**2\n",
    "        size_phis = self.Z_dim * self.U_dim * (self.U_dim - 1)\n",
    "        #size_deltas = self.Z_dim * self.U_dim\n",
    "\n",
    "        m_betas_ind = size_alpha_betas\n",
    "        sigma_betas_ind = 2*size_alpha_betas\n",
    "        m_phis_ind = sigma_betas_ind + size_phis\n",
    "        sigma_phis_ind = m_phis_ind + size_phis\n",
    "        m_omega_all_ind = sigma_phis_ind + (self.U_dim-1) * self.num_teams\n",
    "        sigma_omega_all_ind = m_omega_all_ind + (self.HA_dim) * self.num_teams\n",
    "        rho_ind = sigma_omega_all_ind + (self.HA_dim) * self.num_teams\n",
    "        omegas_all_ind = rho_ind + (self.U_dim-1) * self.num_teams * self.num_gw\n",
    "\n",
    "        m_betas_alpha_opt = x_opt[:m_betas_ind]\n",
    "        sigma_betas_alpha_opt = self.pos_constraint(x_opt[m_betas_ind:sigma_betas_ind])\n",
    "        m_phis_opt = x_opt[sigma_betas_ind:m_phis_ind].reshape(self.Z_dim, self.U_dim, self.U_dim-1)\n",
    "        #sigma_phis_opt = self.pos_constraint(x_opt[m_phis_ind:sigma_phis_ind]).reshape(self.Z_dim, self.U_dim, self.U_dim-1)\n",
    "\n",
    "        m_omegas_all = x_opt[sigma_phis_ind:m_omega_all_ind].reshape(self.num_teams, (self.U_dim-1))\n",
    "        #sigma_omegas_all = self.pos_constraint(x_opt[m_omega_all_ind:sigma_omega_all_ind]).reshape(20, 15)\n",
    "        #rho_all = self.minus_one_one_constraint_np(x_opt[sigma_omega_all_ind:rho_ind]).reshape(20, 15)\n",
    "        #omegas_all = x_opt[rho_ind:omegas_all_ind].reshape(20, 29, 37)\n",
    "\n",
    "        concent_deltas_opt = self.pos_constraint(x_opt[omegas_all_ind:]).reshape(self.Z_dim, self.U_dim)\n",
    "\n",
    "        mode_alpha = np.exp(m_betas_alpha_opt[-1] - sigma_betas_alpha_opt[-1]**2)\n",
    "        mode_betas = np.exp(m_betas_alpha_opt[:-1] - sigma_betas_alpha_opt[:-1]**2).reshape(self.Z_dim, self.U_dim, self.U_dim)\n",
    "\n",
    "        m_phi_plus_omega_all = np.exp(m_phis_opt[:, None, :, :] + m_omegas_all[None, :, None, :]) # Z x teams x M x M-1\n",
    "        denom_mode_all = m_phi_plus_omega_all.sum(-1) + 1 # Z x teams x M\n",
    "\n",
    "        deltas = concent_deltas_opt / concent_deltas_opt.sum(1, keepdims=True)\n",
    "\n",
    "        Gammas = np.zeros((self.Z_dim, self.num_teams, self.U_dim, self.U_dim))\n",
    "        Gammas[:, :, :, :self.U_dim-1] = m_phi_plus_omega_all / denom_mode_all[:, :, :, None]\n",
    "        Gammas[:, :, :, -1] = 1. / denom_mode_all\n",
    "\n",
    "        true_marks_all = []\n",
    "        predicted_marks_all = []\n",
    "        for s in range(self.num_seqs_val):\n",
    "            times_full = self.data_times_val[s]\n",
    "            marks_full = self.data_marks_val[s]\n",
    "            zones_full = self.data_zones_val[s]\n",
    "            teamsID_full = self.data_teamsID_val[s]\n",
    "            size_N = len(times_full)\n",
    "\n",
    "            for ii in range(size_N-1):\n",
    "                times = times_full[:size_N - ii]\n",
    "                marks = marks_full[:size_N - ii]\n",
    "                zones = zones_full[:size_N - ii]\n",
    "                teamsID = teamsID_full[:size_N - ii]\n",
    "\n",
    "                true_marks_all.append(marks[-1])\n",
    "                N = times.size\n",
    "                ar_x = np.arange(N)\n",
    "                prob_all = np.zeros((N, self.U_dim))\n",
    "                prob_all[0] = deltas[zones[0]]\n",
    "                Q_ = min(self.cut_off, N-1)\n",
    "                Q_mtr = np.zeros((Q_, N-1))\n",
    "\n",
    "                for u in range(self.U_dim):\n",
    "                    for q in range(Q_):\n",
    "                        marks_pred = u * np.ones(N - 1 -q, dtype=np.int32)\n",
    "                        tij = times[:N-q-1] - times[q+1:N]\n",
    "                        zones_q = zones[1+q:]\n",
    "                        teamsID_q = teamsID[1+q:]\n",
    "\n",
    "                        Q_mtr[q, q:] = Gammas[zones_q, teamsID_q, marks[:-1-q], marks_pred] * np.exp(mode_betas[zones_q, marks[:-1-q], marks_pred] * tij)\n",
    "\n",
    "                    prob_all[1:, u] = deltas[zones[1:], u] + mode_alpha * Q_mtr.sum(0)\n",
    "                prob_all /= prob_all.sum(axis=1, keepdims=True)\n",
    "\n",
    "                predicted_marks_all.append(np.argmax(prob_all, axis=1)[-1])\n",
    "\n",
    "        true_marks_all, predicted_marks_all = np.array(true_marks_all), np.array(predicted_marks_all)\n",
    "\n",
    "        return accuracy_score(true_marks_all, predicted_marks_all)\n",
    "\n",
    "\n",
    "    def log_likelihood(self, betas_alpha, deltas, phis, omegas_all):\n",
    "        \"\"\"\n",
    "        Compute the value of log-likelihood\n",
    "        \"\"\"\n",
    "        #phis = clamp_preserve_gradients(phis, -5.0, 5.0)\n",
    "        #omegas_all = clamp_preserve_gradients(omegas_all, -5.0, 5.0)\n",
    "\n",
    "        alpha = betas_alpha[-1]\n",
    "        beta_mtr = betas_alpha[:-1].reshape(self.Z_dim, self.U_dim, self.U_dim)\n",
    "        phis_ = phis.reshape(self.Z_dim, self.U_dim, self.U_dim-1)\n",
    "        # omegas_all: 30 x 47 x 80\n",
    "\n",
    "        log_lkl_full = 0.\n",
    "        gameweek = 0\n",
    "        ind_tally = np.zeros(self.num_teams, dtype=int)\n",
    "        ind_gw = np.zeros(self.num_teams, dtype=int)\n",
    "        for s in range(self.num_seqs):\n",
    "            marks = self.data_marks[s]\n",
    "            zones = self.data_zones[s]\n",
    "            teamsID = self.data_teamsID[s]\n",
    "            minus_tau_ij = self.tau_ij_minus_list[s]\n",
    "            N = self.num_jumps[s]\n",
    "\n",
    "            if (s) % 4 == 0 or s == 0:\n",
    "                team1, team2 = list(set(teamsID))\n",
    "                #Need to add 1 to gw of teams in current game, except for first gw where it should remain 0:\n",
    "                ind_tally[team1] += 1\n",
    "                ind_tally[team2] += 1\n",
    "                if ind_tally[team1] >= 2:\n",
    "                    ind_gw[team1] +=1\n",
    "                if ind_tally[team2] >= 2:\n",
    "                    ind_gw[team2] += 1\n",
    "                #Wish to access specific Omegas:\n",
    "                omegas_final = torch.zeros((self.num_teams, (self.U_dim-1)))\n",
    "                for i in range(self.num_teams):\n",
    "                    omegas_final[i] = omegas_all[i, :, ind_gw[i]]\n",
    "\n",
    "                phi_plus_omega_all = phis_[:, None, :, :] + omegas_final[None, :, None, :] # Z x teams x M x M-1\n",
    "                exp_phi_plus_omega_all = phi_plus_omega_all.exp()\n",
    "                denom_all = exp_phi_plus_omega_all.sum(-1) + 1 # Z x teams x M\n",
    "\n",
    "                Gammas = torch.zeros((self.Z_dim, self.num_teams, self.U_dim, self.U_dim))\n",
    "                Gammas[:, :, :, :self.U_dim-1] = exp_phi_plus_omega_all / denom_all[:, :, :, None]\n",
    "                Gammas[:, :, :, -1] = 1. / denom_all\n",
    "\n",
    "\n",
    "            Q_mtr = torch.zeros((self.cut_off, N-1))\n",
    "            Q_denom = torch.zeros((N-1, self.cut_off))\n",
    "            for q in range(self.cut_off):\n",
    "                tij = minus_tau_ij[q, q:]\n",
    "                marks_1_q = marks[:-1-q]\n",
    "                marks_q_1 = marks[1+q:]\n",
    "                zones_q = zones[1+q:]\n",
    "                teamsID_q = teamsID[1+q:]\n",
    "\n",
    "                Q_mtr[q, q:] = Gammas[zones_q, teamsID_q, marks_1_q, marks_q_1] * (beta_mtr[zones_q, marks_1_q, marks_q_1] * tij).exp()\n",
    "                Q_denom[q:, q] = (Gammas[zones_q, teamsID_q, marks_1_q] * (beta_mtr[zones_q, marks_1_q] * tij[:, None]).exp()).sum(1)\n",
    "\n",
    "            numer = deltas[zones[0], marks[0]].log() + (deltas[zones[1:], marks[1:]] + alpha * Q_mtr.sum(0)).log().sum()\n",
    "            denom = (1. + alpha * Q_denom.sum(1)).log().sum()\n",
    "\n",
    "            log_lkl_full += numer - denom\n",
    "\n",
    "        return log_lkl_full\n",
    "\n",
    "\n",
    "    def log_prior(self, omegas_all, m_omegas_all, sigma_omegas_all, rho_all):\n",
    "        \"\"\"\n",
    "        Compute the value of log-prior\n",
    "        \"\"\"\n",
    "        # log-pdf omegas\n",
    "        stacked_rho = torch.hstack((rho_all, rho_all))[:, :-1] #  20 x 29\n",
    "        stacked_sigma = torch.hstack((sigma_omegas_all, sigma_omegas_all))[:, :-1] #  20 x 29\n",
    "        one_rho_sq = 1. - stacked_rho.square() # 20 x 29\n",
    "        diff = omegas_all - m_omegas_all[:, :, None] # 20 x 29 x 37\n",
    "        Lq_diff = torch.zeros_like(diff)\n",
    "        Lq_diff[:, :, -1] = diff[:, :, -1] * one_rho_sq.sqrt()\n",
    "        Lq_diff[:, :, :-1] = diff[:, :, :-1] - stacked_rho[:, :, None] *  diff[:, :, 1:]\n",
    "        Lq_diff /= stacked_sigma[:, :, None]\n",
    "        log_prior = - 0.5 * (Lq_diff.square().sum() + 2 * 2 * self.num_gw * sigma_omegas_all.log().sum() - one_rho_sq.log().sum())\n",
    "\n",
    "        return log_prior\n",
    "\n",
    "\n",
    "    def log_joint(self, samples_betas_alpha, samples_deltas, samples_phis, omegas_all, m_omegas_all, sigma_omegas_all, rho_all):\n",
    "        \"\"\"\n",
    "        Compute log joint\n",
    "        \"\"\"\n",
    "        # Compute the log-likelihood\n",
    "        loglik = self.log_likelihood(samples_betas_alpha, samples_deltas, samples_phis, omegas_all)\n",
    "\n",
    "        # Compute the log-prior\n",
    "        logprior = self.log_prior(omegas_all, m_omegas_all, sigma_omegas_all, rho_all)\n",
    "\n",
    "        return loglik + logprior\n",
    "\n",
    "\n",
    "    def objective(self, x):\n",
    "        \"\"\"\n",
    "        Importance weighted variational objective function\n",
    "        Arguments:\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "             The variational parameters to be optimized\n",
    "        \"\"\"\n",
    "        # Split the parameters into concentration parameters and means/stds\n",
    "        m_betas_alpha = x[:self.m_betas_ind]\n",
    "        sigma_betas_alpha = self.positive_constraint(x[self.m_betas_ind:self.sigma_betas_ind])\n",
    "        m_phis = x[self.sigma_betas_ind:self.m_phis_ind]\n",
    "        sigma_phis = self.positive_constraint(x[self.m_phis_ind:self.sigma_phis_ind])\n",
    "\n",
    "        m_omegas_all = x[self.sigma_phis_ind:self.m_omega_all_ind].reshape(self.num_teams, (self.U_dim-1))\n",
    "        sigma_omegas_all = self.positive_constraint(x[self.m_omega_all_ind:self.sigma_omega_all_ind]).reshape(self.num_teams, (self.HA_dim))\n",
    "        rho_all = self.minus_one_one_constraint(x[self.sigma_omega_all_ind:self.rho_ind]).mul(2.).add(-1.).clamp(self.l_b, self.u_b).reshape(self.num_teams, (self.HA_dim))\n",
    "\n",
    "        omegas_all = x[self.rho_ind:self.omegas_all_ind].reshape(self.num_teams, (self.U_dim-1), self.num_gw)\n",
    "\n",
    "        concent_deltas = self.positive_constraint(x[self.omegas_all_ind:]).reshape(self.Z_dim, self.U_dim)\n",
    "\n",
    "        # Sample noise and Dirichlet distributed probability vectors\n",
    "        samples_betas_alpha = (m_betas_alpha + sigma_betas_alpha * torch.randn(self.num_samples, self.size_alpha_betas)).exp()\n",
    "        samples_phis = m_phis + sigma_phis * torch.randn(self.num_samples, self.size_phis)\n",
    "\n",
    "        dirichlet_deltas = torch.distributions.dirichlet.Dirichlet(concent_deltas)\n",
    "        samples_deltas = dirichlet_deltas.rsample((self.num_samples, )) # num_samples x Z x U\n",
    "\n",
    "        elbo_value = 0.0\n",
    "        # Compute a Monte Carlo estimate of ELBO\n",
    "        for l in range(self.num_samples):\n",
    "            elbo_value += self.log_joint(samples_betas_alpha[l], samples_deltas[l], samples_phis[l], omegas_all, m_omegas_all, sigma_omegas_all, rho_all)\n",
    "        elbo_value /= self.num_samples\n",
    "        return elbo_value\n",
    "\n",
    "\n",
    "    def fit(self, coeffs_0):\n",
    "        if not isinstance(coeffs_0, np.ndarray):\n",
    "            raise TypeError(\"Invalid 'coeffs_0' provided. Coeffs must be a numpy array\")\n",
    "\n",
    "        if coeffs_0.size != self.n_var_params or coeffs_0.ndim != 1:\n",
    "            raise ValueError(\"Invalid size of 'coeffs_0' provided. \\\n",
    "                             'coeffs_0' must be a numpy array of size (\" + str(self.n_var_params) + \", )\")\n",
    "\n",
    "\n",
    "        self.coeffs = torch.tensor(coeffs_0, requires_grad=True)\n",
    "        self.coeffs_old = self.coeffs.detach().clone()\n",
    "\n",
    "        optimizer = torch.optim.Adam([self.coeffs], lr=self.lr_init)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=self.lr_gamma)\n",
    "        self.train_time = 0.\n",
    "        impatient = 0\n",
    "        patience = 200\n",
    "        best_acc = 0.\n",
    "        self.time_of_start = time.time()\n",
    "        for iter in range(self.max_iter):\n",
    "            start_t = time.time()\n",
    "            # Gradient update\n",
    "            optimizer.zero_grad()\n",
    "            elbo_val = -1.0 * self.objective(self.coeffs)\n",
    "            elbo_val.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            self.train_time += time.time() - start_t\n",
    "\n",
    "            self.elbo_history[iter] = -elbo_val.detach().numpy()\n",
    "            acc_val = 100 * self.compute_acc()\n",
    "\n",
    "            if (acc_val - best_acc) < 1e-4:\n",
    "                impatient += 1\n",
    "                if acc_val > best_acc:\n",
    "                    best_acc = acc_val\n",
    "                    self.best_coeffs = self.coeffs.detach().clone()\n",
    "            elif acc_val > best_acc:\n",
    "                best_acc = acc_val\n",
    "                self.best_coeffs = self.coeffs.detach().clone()\n",
    "                impatient = 0\n",
    "\n",
    "            if impatient >= patience:\n",
    "                print(f'Breaking due to early stopping at epoch {iter}')\n",
    "                break\n",
    "\n",
    "            # Check that the optimization did not fail\n",
    "            if torch.isnan(self.coeffs).any():\n",
    "                raise ValueError('NaNs in coeffs! Stop optimization...')\n",
    "\n",
    "            # Convergence check\n",
    "            if self.check_convergence():\n",
    "                if self.verbose:\n",
    "                    self.callback(iter, acc_val, best_acc)\n",
    "                    print('Converged!')\n",
    "                break\n",
    "            elif self.verbose:\n",
    "                self.callback(iter, acc_val, best_acc)\n",
    "\n",
    "        print()\n",
    "        self.time_of_finish = time.time()\n",
    "        self.total_time_taken = self.time_of_finish - self.time_of_start\n",
    "\n",
    "def pos_constraint2(input):\n",
    "    res = np.zeros_like(input)\n",
    "    mask = input > 5\n",
    "    res[mask] = input[mask]\n",
    "    res[~mask] = np.log(1. + np.exp(input[~mask]))\n",
    "    return res\n",
    "\n",
    "def minus_one_one_constraint_np(input):\n",
    "    return (2. / (1. + np.exp(-input)) - 1.).clip(min=-1+1e-6, max=1-1e-6)\n",
    "\n",
    "def compute_scores(times_list, marks_list, zones_list, teamsID_list, alpha, beta_mtr, delta_vec, Gamma_mtr, Q: int=5):\n",
    "    S = len(times_list)\n",
    "    U_dim = delta_vec.shape[1]\n",
    "    true_marks_all = []\n",
    "    predicted_marks_all = []\n",
    "\n",
    "\n",
    "    for s in range(S):\n",
    "        times_full = times_list[s]\n",
    "        marks_full = marks_list[s]\n",
    "        zones_full = zones_list[s]\n",
    "        teamsID_full = teamsID_list[s]\n",
    "        size_N = times_full.size\n",
    "\n",
    "        for ii in range(size_N-1):\n",
    "            times = times_full[:size_N - ii]\n",
    "            marks = marks_full[:size_N - ii]\n",
    "            zones = zones_full[:size_N - ii]\n",
    "            teamsID = teamsID_full[:size_N - ii]\n",
    "\n",
    "            true_marks_all.append(marks[-1])\n",
    "            N = times.size\n",
    "            ar_x = np.arange(N)\n",
    "            prob_all = np.zeros((N, U_dim))\n",
    "            prob_all[0] = delta_vec[zones[0]]\n",
    "            Q_ = min(Q, N-1)\n",
    "            Q_mtr = np.zeros((Q_, N-1))\n",
    "\n",
    "            for u in range(U_dim):\n",
    "                for q in range(Q_):\n",
    "                    marks_pred = u * np.ones(N - 1 -q, dtype=np.int32)\n",
    "                    tij = times[:N-q-1] - times[q+1:N]\n",
    "                    zones_q = zones[1+q:]\n",
    "                    teamsID_q = teamsID[1+q:]\n",
    "\n",
    "                    Q_mtr[q, q:] = Gamma_mtr[zones_q, teamsID_q, marks[:-1-q], marks_pred] * np.exp(beta_mtr[zones_q, marks[:-1-q], marks_pred] * tij)\n",
    "\n",
    "                prob_all[1:, u] = delta_vec[zones[1:], u] + alpha * Q_mtr.sum(0)\n",
    "            prob_all /= prob_all.sum(axis=1, keepdims=True)\n",
    "\n",
    "            predicted_marks_all.append(np.argmax(prob_all, axis=1)[-1])\n",
    "\n",
    "    true_marks_all, predicted_marks_all = np.array(true_marks_all), np.array(predicted_marks_all)\n",
    "    assert true_marks_all.size == predicted_marks_all.size\n",
    "    print(\"  Num predicted:\", predicted_marks_all.size)\n",
    "    return f1_score(true_marks_all, predicted_marks_all, average='micro'), accuracy_score(true_marks_all, predicted_marks_all)\n",
    "\n",
    "\n",
    "def lognorm_pdf(x, mu, sigma):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        assert (x > 0).all()\n",
    "    else:\n",
    "        assert x > 0\n",
    "\n",
    "    return (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2)) / (x * sigma * np.sqrt(2 * np.pi)))\n",
    "\n",
    "\n",
    "def norm_pdf(x, mu, sigma):\n",
    "    return (np.exp(-(x - mu)**2 / (2 * sigma**2)) / (sigma * np.sqrt(2 * np.pi)))\n",
    "\n",
    "\n",
    "def norm_logpdf(x, mu, sigma):\n",
    "    log_exp_term = np.square((x - mu) / sigma)\n",
    "    log_det = np.log(sigma)\n",
    "    return -0.5 * (np.log(2 * np.pi) + log_exp_term) - log_det\n",
    "\n",
    "def lognorm_logpdf(x, mu, sigma):\n",
    "    log_x = np.log(x)\n",
    "    return norm_logpdf(log_x, mu, sigma) - log_x\n",
    "\n",
    "\n",
    "def log_pdf_mixture_lognormals_stable(x, pi_vec, mu_vec, sigma_sq_vec):\n",
    "    assert len(pi_vec) == len(mu_vec) and len(pi_vec) == len(sigma_sq_vec)\n",
    "    assert np.isclose(np.sum(pi_vec), 1.) and (pi_vec >= 0.0).all() and (sigma_sq_vec > 0.0).all()\n",
    "    assert (x >= 0.0).all()\n",
    "    pi_vec[pi_vec <= 1e-8] = 1e-8\n",
    "\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x[x <= 1e-8] = 1e-8\n",
    "    else:\n",
    "        x = 1e-8 if x <= 1e-8 else x\n",
    "    \n",
    "    sigma_vec = np.sqrt(sigma_sq_vec) \n",
    "    log_x = np.log(x)\n",
    "    \n",
    "    weighted_log_prob = np.zeros((len(x), len(pi_vec))) if isinstance(x, np.ndarray) else np.zeros((1, len(pi_vec)))\n",
    "    for component in range(len(pi_vec)):\n",
    "        weighted_log_prob[:, component] = norm_logpdf(log_x, mu_vec[component], sigma_vec[component])\n",
    "\n",
    "    weighted_log_prob += np.log(pi_vec)\n",
    "\n",
    "    return logsumexp(weighted_log_prob, axis=1) - log_x\n",
    "\n",
    "\n",
    "def load_pkl(name, dict_name):\n",
    "    with open(name, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin-1')\n",
    "        num_types = data['dim_process']\n",
    "        data = data[dict_name]\n",
    "        \n",
    "    return data, int(num_types)\n",
    "\n",
    "\n",
    "def opt_logNormal_mixture(time_gap, event_type, num_types: int, num_components: int=2):\n",
    "    time_gap = [tg[1:] for tg in time_gap]\n",
    "    event_type = [et[:-1] for et in event_type]\n",
    "\n",
    "    inter_arriv_full = np.concatenate(time_gap)\n",
    "    inter_arriv_full[inter_arriv_full <= 1e-8] = 1e-8\n",
    "    inter_arriv_full[inter_arriv_full > 150] = 150\n",
    "    marks_inter_full = np.concatenate(event_type)\n",
    "\n",
    "    mean_vec_opt = np.zeros(num_types)\n",
    "    weights_opt = np.zeros((num_types, num_components))\n",
    "    means_opt = np.zeros((num_types, num_components))\n",
    "    variances_opt = np.ones((num_types, num_components))\n",
    "    for u in range(num_types):\n",
    "        data = inter_arriv_full[marks_inter_full == u][:, None]\n",
    "        log_data = np.log(data)\n",
    "\n",
    "        if len(data) > 5:\n",
    "            #print('inter-arrivals mean: %.4f | var: %.4f | max: %.4f | min: %.15f | #(< 1e-4): %d | #events: %d' %(data.mean(), data.var(), data.max(), data.min(), (data < 1e-4).sum(), len(data)))\n",
    "            #model = GeneralMixtureModel.from_samples(LogNormalDistribution, n_components=num_components, X=log_data)\n",
    "            #model.fit(log_data)\n",
    "            timemodel = GaussianMixture(n_components=num_components, n_init=1, init_params=\"k-means++\", tol=1e-6, max_iter=30)\n",
    "            timemodel.fit(log_data)\n",
    "\n",
    "            weights_opt[u] = timemodel.weights_\n",
    "            means_opt[u] = timemodel.means_.flatten()\n",
    "            variances_opt[u] = timemodel.covariances_.flatten()\n",
    "            mean_vec_opt[u] = (weights_opt[u] * np.exp(means_opt[u] + 0.5 * variances_opt[u])).sum()\n",
    "        else:\n",
    "            #print(\"No data!\")\n",
    "            mean_vec_opt[u] = 1e-8\n",
    "            weights_opt[u, 0] = 1.0\n",
    "            means_opt[u, 0] = -8 * np.log(10) #np.log(0.2) - 0.5\n",
    "            \n",
    "            \"\"\"\n",
    "            mean_vec_opt[u] = 0.2\n",
    "            weights_opt[u] = np.ones(num_components) / num_components\n",
    "            mean_vec_opt[u] = (weights_opt[u] * np.exp(-1e5 + 0.5 * 1e-8)).sum()\n",
    "            \"\"\"\n",
    "\n",
    "    return mean_vec_opt, weights_opt, means_opt, variances_opt\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc4dbd-36ad-45d7-9965-d9f4ea89bfd3",
   "metadata": {},
   "source": [
    "Below Runs The Mark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "446fe76a-32e2-4cba-ae15-37bb94c5fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunModel(times,marks,zones,teams,save_as,lr_initial,lr,epochs,q):\n",
    "    if __name__ == \"__main__\":\n",
    "        torch.manual_seed(2105098)\n",
    "        np.random.seed(2105098)\n",
    "        \n",
    "        times_list_train,marks_list_train,teams_list_train,zones_list_train,times_list_val,marks_list_val,teams_list_val,zones_list_val = test_train(times,marks,zones,teams)\n",
    "        model = VI_Framework(times_list_train, marks_list_train, zones_list_train, teams_list_train, \n",
    "                             times_list_val, marks_list_val, zones_list_val, teams_list_val,\n",
    "                             num_samples=1, max_iter=epochs, interval=200, print_every=5, cut_off=q, verbose=True, lr_gamma = lr,lr_init=lr_initial)\n",
    "    \n",
    "        print('                   U:', model.U_dim)\n",
    "        print('                   Z:', model.Z_dim)\n",
    "        print('             teamsID:', model.num_teams)\n",
    "        print('          Train seqs:', len(times_list_train))\n",
    "        print('     Train num_jumps:', sum(model.num_jumps))\n",
    "        print('           Test seqs:', len(times_list_val))\n",
    "        print('      Test num_jumps:', sum(model.num_jumps_val))\n",
    "        \n",
    "        x_0 = np.zeros(int(model.n_var_params))\n",
    "        x_0[:model.m_betas_ind] = np.random.normal(loc=2.1, scale=0.1, size=int(model.size_alpha_betas))\n",
    "        x_0[model.m_betas_ind:model.sigma_betas_ind] = np.log(np.exp(np.clip(np.random.normal(loc=0.2, scale=0.1, size=int(model.size_alpha_betas)), 1e-1, 2.0)) - 1.)\n",
    "        x_0[model.sigma_betas_ind:model.m_phis_ind] = np.random.normal(loc=2.1, scale=0.1, size=int(model.size_phis))\n",
    "        x_0[model.m_phis_ind:model.sigma_phis_ind] = np.log(np.exp(np.clip(np.random.normal(loc=0.2, scale=0.1, size=int(model.size_phis)), 1e-1, 2.0)) - 1.)\n",
    "    \n",
    "        #NEED TO EDIT BELOW:\n",
    "        #UDIM = 48\n",
    "        #NUM TEAMS = 30\n",
    "        #Num Games = 1230 \n",
    "        #Num Gameweeks: 82 \n",
    "        #Playoff Gamweeeks: Up to 4x7=28 - WILL IGNORE PLAYOFFS FOR NOW\n",
    "        \n",
    "        x_0[model.sigma_phis_ind:model.m_omega_all_ind] = np.random.normal(loc=0.5, scale=0.1, size= int(model.U_dim-1)*int(model.num_teams))\n",
    "        x_0[model.m_omega_all_ind:model.sigma_omega_all_ind] = np.random.normal(loc=1.1, scale=0.1, size=int(model.num_teams*(model.HA_dim)))\n",
    "        x_0[model.sigma_omega_all_ind:model.rho_ind] = np.random.normal(loc=0, scale=0.1, size=int(model.num_teams*(model.HA_dim)))\n",
    "        x_0[model.rho_ind:model.omegas_all_ind] = np.random.normal(loc=0, scale=0.1, size=int((model.U_dim-1)*model.num_teams*model.num_gw))\n",
    "        x_0[model.omegas_all_ind:] = np.log(np.exp(np.clip(np.random.normal(loc=0.2, scale=0.1, size=int(model.size_deltas)), 1e-1, 2.0)) - 1.)\n",
    "    \n",
    "        print('Number of parameters:', x_0.size)\n",
    "        print(\"\\nStart training Marks...\")\n",
    "        model.fit(x_0)\n",
    "        print(\"Done!\")\n",
    "        print('Training time: %.4f s' %model.train_time)\n",
    "        print('Total Time To Run:',model.total_time_taken)\n",
    "        x_opt = model.best_coeffs.detach().numpy()\n",
    "        np.save(f'D:/Uni Work/Year 4/Diss/Diss_R/ProjectDiss/{save_as}.npy', x_opt)\n",
    "        np.save(f'C:/Users/joshh/GitFolder/NBAScalableMPP/{save_as}.npy', x_opt)\n",
    "        \n",
    "        print(\"\\nStart Mark Assessment...\")\n",
    "        print(20 * '-' + '\\n            Dataset: NBA Play-By-Play' )\n",
    "    \n",
    "        #Below computes Mark Scores\n",
    "        #Seperating Parameters:\n",
    "        m_betas_alpha_opt = x_opt[:model.m_betas_ind]\n",
    "        sigma_betas_alpha_opt = pos_constraint2(x_opt[model.m_betas_ind:model.sigma_betas_ind])\n",
    "        m_phis_opt = x_opt[model.sigma_betas_ind:model.m_phis_ind].reshape(model.Z_dim, model.U_dim, model.U_dim-1)\n",
    "        sigma_phis_opt = pos_constraint2(x_opt[model.m_phis_ind:model.sigma_phis_ind]).reshape(model.Z_dim, model.U_dim, model.U_dim-1)\n",
    "        \n",
    "        m_omegas_all = x_opt[model.sigma_phis_ind:model.m_omega_all_ind].reshape(model.num_teams, (model.U_dim-1))\n",
    "        sigma_omegas_all = pos_constraint2(x_opt[model.m_omega_all_ind:model.sigma_omega_all_ind]).reshape(model.num_teams, (model.HA_dim))\n",
    "        rho_all = minus_one_one_constraint_np(x_opt[model.sigma_omega_all_ind:model.rho_ind]).reshape(model.num_teams, (model.HA_dim))\n",
    "    \n",
    "        omegas_all = x_opt[model.rho_ind:model.omegas_all_ind].reshape(model.num_teams, (model.U_dim-1), model.num_gw)\n",
    "    \n",
    "        concent_deltas_opt = pos_constraint2(x_opt[model.omegas_all_ind:]).reshape(model.Z_dim, model.U_dim)\n",
    "    \n",
    "        mode_alpha = np.exp(m_betas_alpha_opt[-1] - sigma_betas_alpha_opt[-1]**2)\n",
    "        mode_betas = np.exp(m_betas_alpha_opt[:-1] - sigma_betas_alpha_opt[:-1]**2).reshape(model.Z_dim, model.U_dim, model.U_dim)\n",
    "    \n",
    "        m_phi_plus_omega_all = np.exp(m_phis_opt[:, None, :, :] + m_omegas_all[None, :, None, :]) # Z x teams x M x M-1\n",
    "        denom_mode_all = m_phi_plus_omega_all.sum(-1) + 1 # Z x teams x M\n",
    "    \n",
    "        deltas = concent_deltas_opt / concent_deltas_opt.sum(1, keepdims=True)\n",
    "    \n",
    "        Gammas = np.zeros((model.Z_dim, model.num_teams, model.U_dim, model.U_dim))\n",
    "        Gammas[:, :, :, :model.U_dim-1] = m_phi_plus_omega_all / denom_mode_all[:, :, :, None]\n",
    "        Gammas[:, :, :, -1] = 1. / denom_mode_all\n",
    "    \n",
    "        f1_sc, acc_sc = compute_scores(times_list_val, marks_list_val, zones_list_val, teams_list_val, mode_alpha, mode_betas, deltas, Gammas, Q=5)\n",
    "    \n",
    "        print('Test F1: {:.2f}%  | Test Acc: {:.2f}%' .format(100 * f1_sc, 100 * acc_sc))\n",
    "    \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdd567-e6ee-4cbd-bd76-fc85dfb779a6",
   "metadata": {},
   "source": [
    "Below Runs the Time Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "495c72ea-4c20-4eea-9324-2ac171069ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TimeTrain(times,marks,zones,teams, num_comp, U_dim):\n",
    "    \n",
    "        times_list_train,marks_list_train,teams_list_train,zones_list_train,times_list_val,marks_list_val,teams_list_val,zones_list_val = test_train(times,marks,zones,teams)\n",
    "        #Below Computes Time Scores\n",
    "        #Z_dim, num_types, num_teams = model_ttest.Z_dim, 40, model_ttest.num_teams\n",
    "        Z_dim, num_types, num_teams = 2, U_dim, 30\n",
    "        print(num_types)\n",
    "        time_gap_train = [np.concatenate(([seq[0]], seq[1:] - seq[:-1])) for seq in times_list_train]\n",
    "        time_gap_test = [np.concatenate(([seq[0]], seq[1:] - seq[:-1])) for seq in times_list_val]\n",
    "    \n",
    "        assert sum(len(seq) for seq in time_gap_train) == sum(len(seq) for seq in marks_list_train)\n",
    "    \n",
    "        num_events_train = sum(len(sq) for sq in times_list_train)\n",
    "        num_events_test = sum(len(sq) for sq in times_list_val)\n",
    "    \n",
    "        #print(20 * '-' + '\\n            Dataset: NBA Play-By-Play' )\n",
    "        #print('                  U:', num_types)\n",
    "        #print('   num_events train:', num_events_train)\n",
    "        #print('    num_events test:', num_events_test)\n",
    "        #print('num sequences train:', len(times_list_train))\n",
    "        #print(' num sequences test:', len(times_list_val))\n",
    "    \n",
    "        inter_arriv_full = np.concatenate(time_gap_train + time_gap_test)\n",
    "        mask_zeros = inter_arriv_full <= 1e-8\n",
    "        inter_arriv_full[mask_zeros] = 1e-7\n",
    "        #inter_arriv_full = inter_arriv_full[~mask_zeros]\n",
    "        #print(\"%% inter-arrival < 1e-8: %.4f\" %(100*mask_zeros.mean()))\n",
    "\n",
    "        clip_upper = inter_arriv_full >= 150\n",
    "        inter_arriv_full[clip_upper] = 150\n",
    "        #inter_arriv_full = inter_arriv_full[~clip_upper]\n",
    "        #print(\"%% inter-arrival > 150: %.4f\" %(100*clip_upper.mean()))\n",
    "\n",
    "    \n",
    "        #print(\"Start training Times...\")\n",
    "        start_t = time.time()\n",
    "        log_first_gap_tr = np.log(np.array([time_gap_seq[0] + 1e-8 for time_gap_seq in time_gap_train]))\n",
    "        mu_hat = log_first_gap_tr.mean()\n",
    "        sigma_hat = log_first_gap_tr.std() + 1e-5\n",
    "    \n",
    "        mean_vec_opt, weights_opt, means_opt, variances_opt = opt_logNormal_mixture(time_gap_train, marks_list_train, num_types=num_types, num_components=num_comp)   \n",
    "        print(mean_vec_opt)\n",
    "        train_time = time.time() - start_t\n",
    "        #print(\"Done!\")\n",
    "    \n",
    "        log_lkl_times, se_mean, se_naive = 0., 0., 0.\n",
    "        num_pred = 0\n",
    "        for s in range(len(times_list_val)):\n",
    "            times = times_list_val[s]\n",
    "            marks = marks_list_val[s]\n",
    "            time_gaps = time_gap_test[s]\n",
    "            time_gaps[time_gaps <= 1e-8] = 1e-8\n",
    "            num_pred += marks.size - 1            \n",
    "            for idx in range(1, len(marks)):\n",
    "                true_tau = time_gaps[idx]\n",
    "                se_mean += (true_tau - mean_vec_opt[marks[idx-1]])**2\n",
    "                se_naive += (true_tau - time_gaps[idx-1])**2\n",
    "            log_inc = lognorm_logpdf(time_gaps[0], mu_hat, sigma_hat)\n",
    "            if np.isnan(log_inc):\n",
    "                log_inc = log_past\n",
    "                print(\"1\")\n",
    "            log_lkl_times += log_inc\n",
    "            log_past = log_inc\n",
    "            \n",
    "            time_gaps = time_gaps[1:]\n",
    "            marks = marks[:-1]\n",
    "            for u in range(num_types):\n",
    "                data = time_gaps[marks == u]\n",
    "                log_inc = log_pdf_mixture_lognormals_stable(data, weights_opt[u], means_opt[u], variances_opt[u]).sum()\n",
    "                log_lkl_times += log_inc\n",
    "        log_lkl_times /= num_events_test\n",
    "        #assert jumps_tst == (jumps_tst_se + len(first_time_gaps_test))\n",
    "        \n",
    "        rmse_mean = np.sqrt(se_mean / num_events_test)\n",
    "        rmse_naive = np.sqrt(se_naive / num_events_test)\n",
    "    \n",
    "        #print(\"  Num predicted:\", num_pred)\n",
    "        #print(\"  Training time: %.1f min\" %(train_time / 60.))\n",
    "        print('   Test log-lkl: %.4f nats' %log_lkl_times)\n",
    "        print('      Test RMSE: %.4f' %rmse_mean)\n",
    "        print('Test RMSE naive: %.4f' %rmse_naive)\n",
    "        return rmse_mean, log_lkl_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940bbf34-7423-4c97-a3c0-10e992cfbbd2",
   "metadata": {},
   "source": [
    "The following runs all of the Mark models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fbe516-e888-4b32-b2bc-d8cf205be7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Rebounds\n",
    "model_2022 = RunModel(times_all,marks_all,zones_all,teams_all, '2022x0_test',lr_initial=0.05,lr=0.995,epochs=1000,q=1)\n",
    "model_2022_2 = RunModel(times_all,marks_all,zones_all,teams_all, '2022x0_200',lr_initial=0.05,lr=0.995,epochs=200,q=1)\n",
    "model_2023 = RunModel(times_all_2023,marks_all_2023,zones_all_2023,teams_all_2023, '2023x0_test',lr_initial=0.05,lr=0.995,epochs=1000,q=1)\n",
    "model_2023_2 = RunModel(times_all_2023,marks_all_2023,zones_all_2023,teams_all_2023, '2023x0_200',lr_initial=0.05,lr=0.995,epochs=200,q=1)\n",
    "\n",
    "model_2022_q2 = RunModel(times_all,marks_all,zones_all,teams_all, '2022x0_q2',lr=0.99,epochs=200,q=2, lr_initial = 0.05)\n",
    "model_2022_q3 = RunModel(times_all,marks_all,zones_all,teams_all, '2022x0_q3',lr=0.99,epochs=200,q=3, lr_initial = 0.05)\n",
    "model_2022_q4 = RunModel(times_all,marks_all,zones_all,teams_all, '2022x0_q4',lr=0.99,epochs=200,q=4, lr_initial = 0.05)\n",
    "model_2022_q5 = RunModel(times_all,marks_all,zones_all,teams_all, '2022x0_q5',lr=0.99,epochs=200,q=5, lr_initial = 0.05)\n",
    "model_2022_q10 = RunModel(times_all,marks_all,zones_all,teams_all, '2022x0_q10',lr=0.99,epochs=200,q=10, lr_initial = 0.05)\n",
    "\n",
    "model_2023_q2 = RunModel(times_all_2023,marks_all_2023,zones_all_2023,teams_all_2023, '2023x0_q2',lr=0.99,epochs=200,q=2, lr_initial = 0.05)\n",
    "model_2023_q3 = RunModel(times_all_2023,marks_all_2023,zones_all_2023,teams_all_2023, '2023x0_q3',lr=0.99,epochs=200,q=3, lr_initial = 0.05)\n",
    "model_2023_q4 = RunModel(times_all_2023,marks_all_2023,zones_all_2023,teams_all_2023, '2023x0_q4',lr=0.99,epochs=200,q=4, lr_initial = 0.05)\n",
    "model_2023_q5 = RunModel(times_all_2023,marks_all_2023,zones_all_2023,teams_all_2023, '2023x0_q5',lr=0.99,epochs=200,q=5, lr_initial = 0.05)\n",
    "model_2023_q10 = RunModel(times_all_2023,marks_all_2023,zones_all_2023,teams_all_2023, '2023x0_q10',lr=0.99,epochs=200,q=10, lr_initial = 0.05)\n",
    "\n",
    "#Without Rebounds\n",
    "model_2022_noreb = RunModel(noreb_times_all_2022,noreb_marks_all_2022,noreb_zones_all_2022,noreb_teams_all_2022, '2022x0_noreb',lr_initial=0.05,lr=0.99,epochs=200,q=1)\n",
    "model_2023_noreb = RunModel(noreb_times_all_2023,noreb_marks_all_2023,noreb_zones_all_2023,noreb_teams_all_2023, '2023x0_noreb',lr_initial=0.05,lr=0.99,epochs=200,q=1)\n",
    "\n",
    "model_2023_noreb_q2 = RunModel(noreb_times_all_2022,noreb_marks_all_2022,noreb_zones_all_2022,noreb_teams_all_2022, '2022x0_noreb_q2',lr_initial=0.05,lr=0.99,epochs=250,q=2)\n",
    "model_2023_noreb_q3 = RunModel(noreb_times_all_2022,noreb_marks_all_2022,noreb_zones_all_2022,noreb_teams_all_2022, '2022x0_noreb_q3',lr_initial=0.05,lr=0.99,epochs=250,q=3)\n",
    "\n",
    "\n",
    "model_2023_noreb_q2 = RunModel(noreb_times_all_2023,noreb_marks_all_2023,noreb_zones_all_2023,noreb_teams_all_2023, '2023x0_noreb_q2',lr_initial=0.05,lr=0.99,epochs=250,q=2)\n",
    "model_2023_noreb_q3 = RunModel(noreb_times_all_2023,noreb_marks_all_2023,noreb_zones_all_2023,noreb_teams_all_2023, '2023x0_noreb_q3',lr_initial=0.05,lr=0.99,epochs=250,q=3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84f6c079-0a6a-425a-bbf2-a3b4866e3a3d",
   "metadata": {},
   "source": [
    "Time Components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc6b45-e835-4c1d-81a3-8153d9c120dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeTrain(times_all,marks_all,zones_all,teams_all,1,42)\n",
    "TimeTrain(times_all,marks_all,zones_all,teams_all,2,42)\n",
    "TimeTrain(times_all,marks_all,zones_all,teams_all,3,42)\n",
    "TimeTrain(times_all,marks_all,zones_all,teams_all,4,40)\n",
    "\n",
    "TimeTrain(times_all_2023,marks_all_2023,zones_all_2023,teams_all_2023,1,42)\n",
    "TimeTrain(times_all_2023,marks_all_2023,zones_all_2023,teams_all_2023,2,42)\n",
    "TimeTrain(times_all_2023,marks_all_2023,zones_all_2023,teams_all_2023,3,42)\n",
    "TimeTrain(times_all_2023,marks_all_2023,zones_all_2023,teams_all_2023,4,42)\n",
    "\n",
    "TimeTrain(noreb_times_all_2023,noreb_marks_all_2023,noreb_zones_all_2023,noreb_teams_all_2023,1,40)\n",
    "TimeTrain(noreb_times_all_2023,noreb_marks_all_2023,noreb_zones_all_2023,noreb_teams_all_2023,2,40)\n",
    "TimeTrain(noreb_times_all_2023,noreb_marks_all_2023,noreb_zones_all_2023,noreb_teams_all_2023,3,40)\n",
    "TimeTrain(noreb_times_all_2023,noreb_marks_all_2023,noreb_zones_all_2023,noreb_teams_all_2023,4,40)\n",
    "\n",
    "TimeTrain(noreb_times_all_2022,noreb_marks_all_2022,noreb_zones_all_2022,noreb_teams_all_2022,1,40)\n",
    "TimeTrain(noreb_times_all_2022,noreb_marks_all_2022,noreb_zones_all_2022,noreb_teams_all_2022,2,40)\n",
    "TimeTrain(noreb_times_all_2022,noreb_marks_all_2022,noreb_zones_all_2022,noreb_teams_all_2022,3,40)\n",
    "TimeTrain(noreb_times_all_2022,noreb_marks_all_2022,noreb_zones_all_2022,noreb_teams_all_2022,4,40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
